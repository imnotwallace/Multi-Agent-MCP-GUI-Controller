# Instructions for Updating the Multi-Agent MCP Context Manager

Approach each task one at a time, ensuring clarity and correctness before moving to the next.  After each task, push your changes and document the task completion. 

After completing all tasks, review the entire codebase for consistency and functionality.

1. **1. Update script links** [ ]  

Change all references to ensure everything in the gui scripts are correctly pointing to each other.  For example, run_redesigned_system.py should call redesigned_comprehensive_gui_hub.py instead of redesigned_comprehensive_gui.py.  Ensure all subprocess calls and imports are correctly aligned with the new file structure and naming conventions.

2. **Updated Permission Levels** [ ]

Correct the permission levels to reflect: self, team, session, project.  The former "fuil" permission level is now "session", and a new "project" level has been added that allows access to all contexts within the entire project.

3. **UI update** [ ]

Ensure the GUI reflects the new permission levels in all relevant dropdowns, labels, and documentation.

4. **UI update** [ ]

Update the Project & Sessions screen so that the buttons at the buttom are in two rows instead of one row.  The first row should have the buttons: + New Project, + New Session, Rename.  The second row should have the buttons: Refresh, Delete.

5. **Context DB Schema Update** [ ]

Modify the database schema.  We now have a new table for "context-chunks" that stores individual chunks of context data.  The existing "contexts" table will now store metadata about the context, such as the agent_id, timestamp, session_id, and project_id.  Each entry in the "contexts" table can be linked to multiple entries in the "context-chunks" table via a foreign key relationship.  There should be a one-to-many relationship between "contexts" and "context-chunks".  There shall always be at least one chunk per context, but there can be multiple chunks per context.  The DB schema for "context-chunks" should be as follows:
    - id (Primary Key, Integer, Auto-increment)
    - context_id (Foreign Key to contexts.id, Integer)
    - chunk_index (Integer, indicates the order of the chunk within the context)
    - chunk_content (Text, the actual chunk of context data)
    - agent_id (Text, the agent who created this chunk)
    - session_id (references sessions.id, Integer)
    - project_id (references projects.id, Integer)
    - created_at (Timestamp, default to current timestamp)

6. **UI update** [ ]

On the Contexts tab, the current column that says "Context" should be renamed to "Context Summary".  This column should display the first 100 characters of the first chunk of context.  Add a new column "No. of Chunks" that displays the number of chunks associated with each context.

7. **UI update** [ ]

On the Contexts tab, add a new button "View Chunks" that opens a new window showing all the chunks associated with the selected context.  This window should display each "chunk_content" as a separate row, in sequential order.  The window should also have a button to "Add Chunk" that allows the user to add a new chunk to the selected context.  When adding a chunk, the user should be able to enter the chunk content in a text box, and upon saving, the new chunk should be added to the "context-chunks" table and linked to the selected context.  The character limit for each chunk should be 1150 characters in the UI interface (this is not a database constraint, just a UI constraint).

8. **Chunking Logic** [ ]

Whenever the WriteDB method is called, the following logic should be applied o split context into chunks:

```python
   from langchain.text_splitter import RecursiveCharacterTextSplitter

        # 1. Define your desired chunk size
        CHUNK_SIZE = 1000  # e.g., 1000 characters

        # 2. Calculate the 15% overlap
        OVERLAP_PERCENTAGE = 0.15
        CHUNK_OVERLAP = int(CHUNK_SIZE * OVERLAP_PERCENTAGE) # = 150

        # 3. Instantiate the splitter with the calculated overlap
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP, # This is where the 15% redundancy is set
            length_function=len, # Measures length by characters
        )

        # Use the splitter to generate chunks
        chunks = text_splitter.split_text(your_source_document_text)
```

    The logic should ensure:
        - The code should split sentences into chunks of up to 1150 characters, ensuring no sentence is split across chunks.
        - Each chunk should be as close to 1150 characters as possible without exceeding it.
        - Each chunk should be stored as a sequential entry in the "context-chunks" table, linked to the parent context.
        - The WriteDB method should return a success response only after all chunks have been successfully stored in the database.  
        - After the success response is sent, the MCP server should then commence a new method "VectoriseChunks" that takes the newly created chunks and vectorises them using vectorisation logic (placeholder for now).  This method should run asynchronously in the background and not block the WriteDB response.

9. **ReadDB Method Update** [ ]

Update the ReadDB method to retrieve the last 10 chunks associated with the agent's contexts and their permissions, ordered by created_at descending.  The method should return these chunks in a JSON array format, including the chunk_content (sorted in by context_id, then chunk_index in sequential order) and metadata (agent_id, session_id, created_at).  The ReadDB method should also accept an optional parameter "since" which is a timestamp.  If provided, only the 10 chunks created chronologically after this timestamp should be returned, that fit the permission levels for the agent.  The args should be as follows:

```json
   {
     "method": "ReadDB",
     "params": {
       "agent_id": "my_agent",
       "since": "2023-10-01T00:00:00Z",  // Optional ISO 8601 timestamp, if null then retrieve 10 most recent chunks according to agent_id and permissions
       "permission_level" : "session" // Optional, one of ["self", "team", "session", "project"].  Defaults to "null" if not provided, which means the agent's default permission level from the DB is used.
     }
   }
```

10. **Create sqlite-vss table** [ ]

Using the sqlite-vss extension, create a new Virtual Table named "context_chunk_embeddings" (using underscores for consistency with other table names) to store vector embeddings for each chunk. The table should be created via a migration script. The schema for the virtual table should be as follows:

- chunk_id (Primary Key, Integer, references context-chunks.id)
- embedding (Vector, 256 dimensions)

11. **Index the embeddings** [ ]

Ensure that the vector embeddings are indexed for efficient similarity search. Use the appropriate indexing method provided by sqlite-vss for 256-dimensional vectors. Ensure that the index is created when the server starts up if it does not already exist.

12. **VectoriseChunks Method** [ ]

Implement a new method "VectoriseChunks" that takes a list of chunk IDs and vectorises them using a placeholder vectorisation function. This method should be called automatically after WriteDB completes successfully, passing the IDs of the newly created chunks. The method should run asynchronously in the background and not block the WriteDB response. The method signature should be as follows:

```json
   {
     "method": "VectoriseChunks",
     "params": {
       "chunk_ids": [1, 2, 3, ...] // List of chunk IDs to vectorise
     }
   }
```

    The method should:
        - Log the start and completion of the vectorisation process for monitoring purposes.
        - To vectorise, we need to use the nomic.ai embedding model. When the server initialises, it must load the nomic embedding model as follows:

```python
        from nomic import embed
        import numpy as np

        output = embed.text(
            texts=["Initialise nomic embedding model"],
            model='nomic-embed-text-v1.5',
            task_type="search_document",
            inference_mode='local',
            dimensionality=256,
        )

        print(output['Initialise nomic embedding model'])

        embeddings = np.array(output['embeddings'])
```

    This initial call loads the model into memory.  We don't want to store this initial output.  This initialisation must occur before any ansynchronous VecoriseDB call is made.  After this, we can call the model to vectorise text chunks.
    
    This is a local model that we can call via a Python function.  The function is as follows:

```python
   def vectorise_text_chunks(chunks):
       from nomic import embed
       import numpy as np

       output = embed.text(
           texts=chunks,
           model='nomic-embed-text-v1.5',
           task_type="search_document",
           inference_mode='local',
           dimensionality=256,
       )

       print(output['usage'])

       embeddings = np.array(output['embeddings'])
       return embeddings
```

    These embeddings can then be stored in the "context_chunk_embeddings" virtual table, linked to the corresponding chunk_id.

13. **Index DB** [ ]

Index the "context_id" column in the "context-chunks" table to optimize queries that retrieve chunks by their parent context.  Index the "created_at" column in the "context-chunks" table to optimize queries that filter chunks based on their creation timestamp.
